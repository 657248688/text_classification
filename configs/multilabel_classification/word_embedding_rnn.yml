# global parameters
global_parameters:
  # model related
  - &model_arch_type 'RnnModel'  # 可选的模型['FastText'，'TextCNN'，'TextCNN1D'...]
  - &train true #[true,false] # 如果不使用transformers model，该参数表示是否训练词向量，如果使用transformers model，该参数表示是否对transformers model 进行微调
  - &dropout 0.5
  - &class_num 6
  # data related
  - &dataset_type 'MedicalEmbeddingDataset'
  - &data_dir 'data/medical_question/train_valid' # 定义锚
  - &cache_dir 'data/medical_question/.cache'
  - &overwrite_cache true
  - &word_embedding 'data/word_embedding/.cache/medical_word_embedding.pkl' # [cnews_word_embedding.pkl, cnews_char_embedding.pkl]
  - &num_workers 32
  - &batch_size 64


experiment_name: *model_arch_type
num_gpu: 2                         # GPU数量
device_id: '0,1'
visual_device: '0,1'
main_device_id: '0'
resume_path: null                         # path to latest checkpoint

# 模型
model_arch:
  type: *model_arch_type
  args:
    word_embedding: *word_embedding
    train: *train
    dropout: *dropout
    # rnn parameters
    rnn_type: 'lstm'  # [rnn,lstm,gru]
    hidden_dim: 256
    n_layers: 2
    bidirectional: true
    batch_first: true
    class_num: *class_num

train_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'medical_train.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    word_embedding: *word_embedding
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

valid_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'medical_valid.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    word_embedding: *word_embedding
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker



optimizer:
  type: 'Adam'
  args:
    lr: 0.01

lr_scheduler:
  type: 'get_linear_schedule_with_warmup'
  args:
    num_warmup_steps: 0

loss:
  - "binary_loss"

metrics:
  - 'accuracy'
  - 'macro_precision'
  - 'macro_recall'
  - 'macro_f1'
  - 'micro_precision'
  - 'micro_recall'
  - 'micro_f1'

trainer:
  epochs: 100
  save_dir: 'saved/'
  save_period: 1
  verbosity: 2
  monitor: "max val_macro_f1"
  early_stop: 5
  tensorboard: true

